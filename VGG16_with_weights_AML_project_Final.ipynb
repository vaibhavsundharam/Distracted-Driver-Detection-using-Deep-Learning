{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16 with weights_AML_project_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9823095a1a3242dcb4cea4231ca780be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ac429b5e73814e7cbf65def3771ed625",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7be34101ade14121abc5384714d7161f",
              "IPY_MODEL_39567fdef3fb4d998b82da68cf12b5a5"
            ]
          }
        },
        "ac429b5e73814e7cbf65def3771ed625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7be34101ade14121abc5384714d7161f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e801b21e78254918b840df6384678771",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e78ff974aec4ddb89dcd24c53a8c14a"
          }
        },
        "39567fdef3fb4d998b82da68cf12b5a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9deb02a4983e4ebdb684c00853f104e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:09&lt;00:00, 57.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b31f4c00e76a4ab7b69266775950f10f"
          }
        },
        "e801b21e78254918b840df6384678771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e78ff974aec4ddb89dcd24c53a8c14a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9deb02a4983e4ebdb684c00853f104e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b31f4c00e76a4ab7b69266775950f10f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhavsundharam/Distracted-Driver-Detection-using-Deep-Learning/blob/master/VGG16_with_weights_AML_project_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_bhTuiSjmQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from skimage.transform import resize\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as ff\n",
        "from PIL import Image\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ox-Mf9ttQ-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "21a7f962-b802-4f6c-878a-e012ea592b0f"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvT32aAgtSYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d92997d0-7ac6-4241-fd88-932026ce8e0c"
      },
      "source": [
        "# Set device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB2F9HJmTkTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3a73e356-16f8-4bf7-eaf5-878ed3309614"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxyUEjWNTmXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c16c56a5-dc64-4810-a057-ea79dc11e1c7"
      },
      "source": [
        "prefix = '/content/gdrive/My Drive/'\n",
        "# modify \"customized_path_to_your_project\" \n",
        "customized_path_to_your_project = 'AML/Project/'\n",
        "sys_path = os.path.join(prefix, customized_path_to_your_project)\n",
        "sys.path.append(sys_path)\n",
        "print(f\"System path: {sys_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "System path: /content/gdrive/My Drive/AML/Project/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDTuXjRhUVAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "1556a640-316c-4835-b615-4a084c492f1b"
      },
      "source": [
        "# reading the CSV file with driver-class mapping for training\n",
        "root_dir = sys_path+\"Coding\"\n",
        "driver_imgs_csv = sys_path+\"Coding/CSV/driver_imgs_list.csv\"\n",
        "\n",
        "print(\"Root directory Location: {}\".format(root_dir)+\"\\n\")\n",
        "print(\"CSV file Location: {}\".format(driver_imgs_csv)+\"\\n\")\n",
        "\n",
        "\n",
        "drivers_imgs_file = pd.read_csv(driver_imgs_csv)\n",
        "print(drivers_imgs_file)\n",
        "\n",
        "n=0\n",
        "img_name=drivers_imgs_file.iloc[n,2]\n",
        "img_class=drivers_imgs_file.iloc[n,1]\n",
        "print(\"Image name: {}\".format(img_name))\n",
        "print(\"Image class: {}\".format(img_class[1]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root directory Location: /content/gdrive/My Drive/AML/Project/Coding\n",
            "\n",
            "CSV file Location: /content/gdrive/My Drive/AML/Project/Coding/CSV/driver_imgs_list.csv\n",
            "\n",
            "      subject classname            img\n",
            "0        p002        c0  img_44733.jpg\n",
            "1        p002        c0  img_72999.jpg\n",
            "2        p002        c0  img_25094.jpg\n",
            "3        p002        c0  img_69092.jpg\n",
            "4        p002        c0  img_92629.jpg\n",
            "...       ...       ...            ...\n",
            "22419    p081        c9  img_56936.jpg\n",
            "22420    p081        c9  img_46218.jpg\n",
            "22421    p081        c9  img_25946.jpg\n",
            "22422    p081        c9  img_67850.jpg\n",
            "22423    p081        c9   img_9684.jpg\n",
            "\n",
            "[22424 rows x 3 columns]\n",
            "Image name: img_44733.jpg\n",
            "Image class: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU0MzTaoAcPn",
        "colab_type": "text"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvaHpxhqYpyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3a348124-9067-4b68-b080-f186c1cd8ac9"
      },
      "source": [
        "# number of unique drivers in the dataset\n",
        "unique_driver_list = list(drivers_imgs_file.iloc[:, 0][1:].unique())\n",
        "print(\"Unique drivers are:\", unique_driver_list)\n",
        "print(\"Number of unique drivers:\",len(unique_driver_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique drivers are: ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081']\n",
            "Number of unique drivers: 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FycJ29WJDcJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e7e33707-c2a0-419a-c478-40cec2528813"
      },
      "source": [
        "# Accessing the csv location of each unique driver\n",
        "#print(root_dir)\n",
        "unique_driver_csv_path=list()\n",
        "for driver in unique_driver_list:\n",
        "  driver_csv_path=root_dir+\"/CSV/\"+driver+\".csv\"\n",
        "  unique_driver_csv_path.append(driver_csv_path)\n",
        "print(\"CSV path of unique drivers:\",unique_driver_csv_path, len(unique_driver_csv_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CSV path of unique drivers: ['/content/gdrive/My Drive/AML/Project/Coding/CSV/p002.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p012.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p014.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p015.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p016.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p021.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p022.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p024.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p026.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p035.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p039.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p041.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p042.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p045.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p047.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p049.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p050.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p051.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p052.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p056.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p061.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p064.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p066.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p072.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p075.csv', '/content/gdrive/My Drive/AML/Project/Coding/CSV/p081.csv'] 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCvj4Ak3Dror",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ec8a564b-66ae-4093-a1df-20c2edeb6058"
      },
      "source": [
        "'''\n",
        "import random\n",
        "drivers_val=random.sample(unique_driver_csv_path,5) # A list containing the file location of randomly choosen validation drivers\n",
        "print(\"Drivers seleted for validation\\n\")\n",
        "for i in range(len(drivers_val)):\n",
        "  print(\"Driver name:\",drivers_val[i][49:52],\"---> Driver CSV location:\",drivers_val[i])\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "drivers_train=list() # A list containing the file location of remaining drivers (used for testing)\n",
        "print(\"Drivers selected for testing\\n\")\n",
        "for j in unique_driver_csv_path:\n",
        "  if j in drivers_val:\n",
        "    continue\n",
        "  else:\n",
        "    drivers_train.append(j)\n",
        "    print(\"Driver name:\",j[49:52],\"---> Driver CSV location:\",j)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport random\\ndrivers_val=random.sample(unique_driver_csv_path,5) # A list containing the file location of randomly choosen validation drivers\\nprint(\"Drivers seleted for validation\\n\")\\nfor i in range(len(drivers_val)):\\n  print(\"Driver name:\",drivers_val[i][49:52],\"---> Driver CSV location:\",drivers_val[i])\\n\\nprint(\"\\n\")\\n\\ndrivers_train=list() # A list containing the file location of remaining drivers (used for testing)\\nprint(\"Drivers selected for testing\\n\")\\nfor j in unique_driver_csv_path:\\n  if j in drivers_val:\\n    continue\\n  else:\\n    drivers_train.append(j)\\n    print(\"Driver name:\",j[49:52],\"---> Driver CSV location:\",j)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX8MDjeIQacM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0178d88a-0ad3-4ad9-b81b-cc816dd47762"
      },
      "source": [
        "'''# combining CSV files for test set and validation set\n",
        "\n",
        "# Training data\n",
        "train_combined=pd.concat([pd.read_csv(f) for f in drivers_train])\n",
        "train_combined.to_csv( \"train_combined.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "# Validation data\n",
        "val_combined=pd.concat([pd.read_csv(f) for f in drivers_val])\n",
        "val_combined.to_csv( \"val_combined.csv\", index=False, encoding='utf-8-sig')'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# combining CSV files for test set and validation set\\n\\n# Training data\\ntrain_combined=pd.concat([pd.read_csv(f) for f in drivers_train])\\ntrain_combined.to_csv( \"train_combined.csv\", index=False, encoding=\\'utf-8-sig\\')\\n\\n# Validation data\\nval_combined=pd.concat([pd.read_csv(f) for f in drivers_val])\\nval_combined.to_csv( \"val_combined.csv\", index=False, encoding=\\'utf-8-sig\\')'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2QyzM6Me3Se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DistractedDriverDataset(Dataset): # custom class to explore the dataset \n",
        "\n",
        "  def __init__(self, csv_file_loc, root_dir, transform=None):\n",
        "    self.driver_imgs_file = pd.read_csv(csv_file_loc) #  reading the CSV file\n",
        "    self.root_dir = root_dir # root directory of the images \n",
        "    self.transform = transform # transformations if any\n",
        "\n",
        "  def __len__(self):  # returns the length of the entire training set\n",
        "    return len(self.driver_imgs_file)\n",
        "\n",
        "  def __getitem__(self, idx):    \n",
        "    img_name=os.path.join(self.root_dir,self.driver_imgs_file.iloc[idx,1],self.driver_imgs_file.iloc[idx,2]) # featching the file name    \n",
        "     \n",
        "    driver_name=self.driver_imgs_file.iloc[idx,0] # driver name\n",
        "    #print(img_name, driver_name)\n",
        "    img_tensor=self.transform(Image.open(img_name)) # reading the image\n",
        "    \n",
        "    img_class=int(self.driver_imgs_file.iloc[idx,1][1])# fetching the image class \n",
        "    \n",
        "    return img_tensor,img_class #,img_name[64:77],driver_name\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvoJza92Wich",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a6a3fc17-f309-437c-8073-abc94d18191d"
      },
      "source": [
        "# Here we create two seperate instances of the Distracted driver data set. One will hold the data set for training and the other will hold data for validation\n",
        "\n",
        "train_data_loc=\"/content/train_combined.csv\" # location of the training csv file\n",
        "val_data_loc=\"/content/val_combined.csv\" # location of the validation csv file\n",
        "print(\"Root directory:\",root_dir,\"\\n\")\n",
        "\n",
        "train_dataset=DistractedDriverDataset(csv_file_loc=train_data_loc,root_dir=root_dir+\"/Data/Train\",\n",
        "                                transform=transforms.Compose([transforms.RandomRotation(degrees=(-15,15)), transforms.Resize((244,244)), transforms.ToTensor()]))\n",
        "len_train_dataset=len(train_dataset)\n",
        "print(\"Total number of elements in training data:\", len_train_dataset)\n",
        "\n",
        "\n",
        "val_dataset=DistractedDriverDataset(csv_file_loc=val_data_loc,root_dir=root_dir+\"/Data/Train\",\n",
        "                                transform=transforms.Compose([transforms.Resize((244,244)),transforms.ToTensor()]))\n",
        "len_val_dataset=len(val_dataset)\n",
        "print(\"Total number of elements in validation data:\", len_val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root directory: /content/gdrive/My Drive/AML/Project/Coding \n",
            "\n",
            "Total number of elements in training data: 18661\n",
            "Total number of elements in validation data: 3763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSNqnecyzn8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5db4ab3b-5834-4643-b506-66711ee9cd7a"
      },
      "source": [
        "# claculating the weights for each class\n",
        "\n",
        "train_csv=pd.read_csv(train_data_loc)\n",
        "train_csv.head()\n",
        "count_list=train_csv.groupby('classname').size().to_list()\n",
        "weights=list()\n",
        "print(f\"Counts for each class: {count_list}\")\n",
        "total=sum(count_list)\n",
        "for count in count_list:\n",
        "  w=max(count_list)/count\n",
        "  weights.append(w)\n",
        "\n",
        "weights=torch.FloatTensor(weights).to(device)\n",
        "print(f\"Training weights: {weights}\")\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counts for each class: [2067, 1911, 1967, 1944, 1947, 1916, 1950, 1632, 1558, 1769]\n",
            "Training weights: tensor([1.0000, 1.0816, 1.0508, 1.0633, 1.0616, 1.0788, 1.0600, 1.2665, 1.3267,\n",
            "        1.1685], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6t30_j65gsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c894e86-57f1-4283-c4d0-3c23a4ce60fe"
      },
      "source": [
        "# loading training and validation data for the neural network\n",
        "train_loader=DataLoader(train_dataset, batch_size=32,shuffle=True, num_workers=16) # x.shape--->torch.Size([32, 3, 244, 244])\n",
        "val_loader=DataLoader(val_dataset, batch_size=32,shuffle=True, num_workers=16) # x.shape---> torch.Size([32, 3, 244, 244]) \n",
        "\n",
        "'''for x,y in train_loader:\n",
        "  print(x.shape)\n",
        "  x=x.squeeze()\n",
        "  print(x.shape)\n",
        "  plt.imshow(  x.permute(1, 2, 0)  )\n",
        "  break'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for x,y in train_loader:\\n  print(x.shape)\\n  x=x.squeeze()\\n  print(x.shape)\\n  plt.imshow(  x.permute(1, 2, 0)  )\\n  break'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWnX4x8OSFTZ",
        "colab_type": "text"
      },
      "source": [
        "#Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlTOws0MR_-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function for computing accuracy\n",
        "def get_acc(pred, y):\n",
        "  pred = pred.float()\n",
        "  y = y.float() \n",
        "  return (y==pred).sum().float()/y.size(0)*100.\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbOnQR3TcJfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_acc_and_loss(cal_loss,model,criterion,loader):      \n",
        "  ys=[]\n",
        "  y_preds=[]\n",
        "  loss_total=0\n",
        "  for x, y in loader:    \n",
        "    x=x.to(device)\n",
        "    y=y.to(device)\n",
        "    y_pred=model(x)\n",
        "    ys.append(y)\n",
        "    y_preds.append(torch.argmax(y_pred, dim=1))\n",
        "\n",
        "    if cal_loss==True:\n",
        "      loss=criterion(y_pred,y)\n",
        "      loss_total += loss.item()\n",
        "\n",
        "  if cal_loss==True:    \n",
        "    y=torch.cat(ys, dim=0)\n",
        "    y_pred=torch.cat(y_preds, dim=0)\n",
        "    return get_acc(y_pred, y), loss_total\n",
        "  else:\n",
        "    y=torch.cat(ys, dim=0)\n",
        "    y_pred=torch.cat(y_preds, dim=0)\n",
        "    return get_acc(y_pred, y) \n",
        "  \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXdyeG1mTm6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# solver function for NN training\n",
        "def Solver_NN (model, name, train_loader, val_loader, optim, criterion, len_train_dataset, len_val_dataset, epoch=51, lr=1e-1):\n",
        "  print(\"Solver Initiated\")\n",
        "  model=model.to(device) # sending model to GPU\n",
        "  print(\"Model successfully sent to the GPU\\n\")  \n",
        "\n",
        "  train_acc_list=list()\n",
        "  val_acc_list=list()\n",
        "  epoch_list=list()\n",
        "  train_loss_list=list()\n",
        "  val_loss_list=list()\n",
        "  total_step = len(train_loader)\n",
        "  best_epoch=None\n",
        "  #curr_lr = learning_rate\n",
        "  counter=0\n",
        "  for e in range(epoch):\n",
        "    model.train()\n",
        "    loss_epoch=0     \n",
        "    epoch_list.append(e)   \n",
        "\n",
        "    for i,(x,y) in enumerate(train_loader):      \n",
        "      x = x.to(device)\n",
        "      y = y.to(device)    \n",
        "      #print(\"x is:\",x.shape)     \n",
        "      #print(\"y is:\",y.shape)\n",
        "\n",
        "      \n",
        "\n",
        "      #forward pass\n",
        "      y_pred=model(x)\n",
        "      #print(\"y_pred is:\",y_pred.shape)\n",
        "      loss=criterion(y_pred,y)\n",
        "      #print(\"loss:\",loss)\n",
        "\n",
        "      #bacward pass\n",
        "      optim.zero_grad()\n",
        "      #loss.to(device)\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      \n",
        "      loss_epoch += float(loss.item())\n",
        "      #print(\"loss epoch:\",loss_epoch)    \n",
        "         \n",
        "      #print(f\"Training loss in epoch {e} is {(loss_epoch/len_train_dataset)*1000}\") \n",
        "      #train_loss_list.append((loss_epoch/len_train_dataset)*1000)\n",
        "\n",
        "      if (i+1) % 100 == 0:\n",
        "        print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                   .format(counter+1, epoch, i+1, total_step, loss.item()))\n",
        "    counter+=1\n",
        "      \n",
        "    print(f\"Training loss in epoch {e} is {(loss_epoch/len_train_dataset)*1000}\") \n",
        "    train_loss_list.append((loss_epoch/len_train_dataset)*1000)\n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      val_acc, val_loss = get_model_acc_and_loss(True,model,criterion,val_loader)\n",
        "      print(f\"Validation loss in epoch {e} is {(val_loss/len_val_dataset)*1000}\")\n",
        "      val_loss_list.append((val_loss/len_val_dataset)*1000)\n",
        "      train_acc = get_model_acc_and_loss(False,model,criterion,train_loader)\n",
        "      train_acc_list.append(train_acc)\n",
        "      val_acc_list.append(val_acc)\n",
        "      print(f'Validation accuracy: {val_acc}, Train accuracy: {train_acc}') \n",
        "\n",
        "      if best_epoch==None:\n",
        "        best_epoch=e        \n",
        "        best_val=val_acc\n",
        "        torch.save(model.state_dict(), root_dir+'/Saved Weights/'+name+\" epoch \"+str(e))\n",
        "        print(\"File saved successfully\\n\")\n",
        "      elif val_acc>best_val:\n",
        "        best_epoch=e        \n",
        "        best_val=val_acc\n",
        "        torch.save(model.state_dict(), root_dir+'/Saved Weights/'+name+\" epoch \"+str(e))\n",
        "        print(\"File saved successfully\\n\")\n",
        "      else:\n",
        "        print(\"\\n\")\n",
        "\n",
        "  print(f'Best epoch:{best_epoch}, Best Validtion accuracy:{best_val}')\n",
        "\n",
        "  fig, axs = plt.subplots(2)  \n",
        "  fig.tight_layout(pad=3.0)\n",
        "  axs[0].plot(epoch_list,train_acc_list, 'r--' ,epoch_list,val_acc_list,'g')\n",
        "  axs[0].set_title('Accuracy vs. epochs')  \n",
        "  axs[1].plot(epoch_list,train_loss_list, 'r--' ,epoch_list,val_loss_list,'g')\n",
        "  axs[1].set_title('Loss vs. epochs')\n",
        "  \n",
        "\n",
        "  plt.savefig(name+\".png\")\n",
        "\n",
        "  \n",
        "  \n",
        "  return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x47q8d1UlJF",
        "colab_type": "text"
      },
      "source": [
        "#Neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmoB9E79xkY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Construct a CNN with 2 layers of convolutional layer and 1 layer of linear layer.\n",
        "        self.cnn= nn.Sequential(\n",
        "                            nn.Conv2d(3, 8, kernel_size=5, stride=2, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.MaxPool2d(2,2),\n",
        "                            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.MaxPool2d(2,2),\n",
        "                            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "                            nn.ReLU(),\n",
        "                            nn.MaxPool2d(2,2)\n",
        "                           \n",
        "                    )\n",
        "        self.fc = nn.Linear(2304, 10)\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        return self.fc(self.cnn(x).view(batch_size, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys5FJtPmU6lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class extra_layer(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()               \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 512),\n",
        "            nn.Linear(512, 10))\n",
        "    def forward(self, x):        \n",
        "        return self.fc(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unf1dyOWBzIK",
        "colab_type": "text"
      },
      "source": [
        "#Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rSAsHgxDndx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9823095a1a3242dcb4cea4231ca780be",
            "ac429b5e73814e7cbf65def3771ed625",
            "7be34101ade14121abc5384714d7161f",
            "39567fdef3fb4d998b82da68cf12b5a5",
            "e801b21e78254918b840df6384678771",
            "0e78ff974aec4ddb89dcd24c53a8c14a",
            "9deb02a4983e4ebdb684c00853f104e3",
            "b31f4c00e76a4ab7b69266775950f10f"
          ]
        },
        "outputId": "8d812977-d77e-4e44-dadb-51012f27fae1"
      },
      "source": [
        "# Instantiating one of the models given in Neural Network section\n",
        "\n",
        "def select_model(inp,CNNClassifier, training, extra_layer):\n",
        "  print(f\"Pre trainined: {training}\")\n",
        "  if inp==1:\n",
        "    model=CNNClassifier()\n",
        "    name=\"CNN\"\n",
        "  elif inp==2:\n",
        "    model=nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=training), extra_layer)\n",
        "    name=\"VGG16\"\n",
        "  elif inp==3:\n",
        "    model = nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=training), extra_layer)\n",
        "    name=\"RESNET18\"\n",
        "  elif inp==4:\n",
        "    model = nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', pretrained=training), extra_layer)\n",
        "    name=\"RESNET34\"\n",
        "  elif inp==5:\n",
        "    model=nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=training), extra_layer)\n",
        "    name=\"ALEXNET\"\n",
        "  elif inp==6:\n",
        "    model=nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'mobilenet_v2', pretrained=training), extra_layer)\n",
        "    name=\"MOBILENET_V2\"\n",
        "  return model, name \n",
        "\n",
        "print(\"-------------------Select Model----------------------\")\n",
        "print(\"Input '1' for CNNClassifier\")\n",
        "print(\"Input '2' for VGG16\")\n",
        "print(\"Input '3' for Resnet18\")\n",
        "print(\"Input '4' for Resnet34\")\n",
        "print(\"Input '5' for Alexnet\")\n",
        "print(\"Input '6' for Mobilenet_v2\")\n",
        "inp=int(input(\"Your input---> \"))\n",
        "\n",
        "if inp==2 or inp ==3 or inp ==4 or inp==5 or inp==6:\n",
        "  train=int(input(\"Do you want a pretrained version 1/0? \"))\n",
        "  if train ==1:\n",
        "    pretrained=True\n",
        "  else:\n",
        "    pretrained=False\n",
        "print(\"-----------------------------------------------------------\")\n",
        "\n",
        "model, name=select_model(inp,CNNClassifier(),pretrained,extra_layer())\n",
        "print(f\"selected model---> {name}\\n  {model}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------Select Model----------------------\n",
            "Input '1' for CNNClassifier\n",
            "Input '2' for VGG16\n",
            "Input '3' for Resnet18\n",
            "Input '4' for Resnet34\n",
            "Input '5' for Alexnet\n",
            "Input '6' for Mobilenet_v2\n",
            "Your input---> 2\n",
            "Do you want a pretrained version 1/0? 1\n",
            "-----------------------------------------------------------\n",
            "Pre trainined: True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/archive/v0.6.0.zip\" to /root/.cache/torch/hub/v0.6.0.zip\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9823095a1a3242dcb4cea4231ca780be",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "selected model---> VGG16\n",
            "  Sequential(\n",
            "  (0): VGG(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (6): ReLU(inplace=True)\n",
            "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): ReLU(inplace=True)\n",
            "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): ReLU(inplace=True)\n",
            "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (15): ReLU(inplace=True)\n",
            "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (18): ReLU(inplace=True)\n",
            "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (20): ReLU(inplace=True)\n",
            "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (22): ReLU(inplace=True)\n",
            "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (25): ReLU(inplace=True)\n",
            "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (27): ReLU(inplace=True)\n",
            "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (29): ReLU(inplace=True)\n",
            "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): Dropout(p=0.5, inplace=False)\n",
            "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (1): extra_layer(\n",
            "    (fc): Sequential(\n",
            "      (0): ReLU()\n",
            "      (1): Linear(in_features=1000, out_features=512, bias=True)\n",
            "      (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8N2CRBOr8Jo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "73e16161-7d44-49a1-fe3d-d7ff1d49f824"
      },
      "source": [
        "response=int(input(\"Do you want to load weights from previously saved mode to continue training 1/0 ?\"))\n",
        "if response==1:\n",
        "  weights=str(input(\"Input the location of the pickle file\"))\n",
        "  model.load_state_dict(torch.load(weights))\n",
        "elif response==0:\n",
        "  print(\"No weights loaded\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Do you want to load weights from previously saved mode to continue training 1/0 ?0\n",
            "No weights loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjH2IW1EVF6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#criterion and optimizer\n",
        "\n",
        "lr=1e-2\n",
        "criterion=nn.CrossEntropyLoss(reduction='mean', weight=weights) # loss criterion \n",
        "optim=torch.optim.SGD(model.parameters(), lr=lr) # optimizer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtMETZtbRpGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "54815f12-7fa6-4345-a485-0a79b37e3bb3"
      },
      "source": [
        "# GPU Access and model parameters\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  print(\"Running on Cuda\")\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(\"Learnable parameters:\",params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on Cuda\n",
            "Learnable parameters: 138875186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRvkKt19VHeP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ead0b2ca-3c21-426f-9379-6abb2e6355e5"
      },
      "source": [
        "#model training\n",
        "\n",
        "modle=Solver_NN(model, name, train_loader, val_loader, optim,criterion, len_train_dataset, len_val_dataset, epoch=10, lr=lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solver Initiated\n",
            "Model successfully sent to the GPU\n",
            "\n",
            "Epoch [1/10], Step [100/584] Loss: 1.7140\n",
            "Epoch [1/10], Step [200/584] Loss: 0.4207\n",
            "Epoch [1/10], Step [300/584] Loss: 0.5998\n",
            "Epoch [1/10], Step [400/584] Loss: 0.1439\n",
            "Epoch [1/10], Step [500/584] Loss: 0.1460\n",
            "Training loss in epoch 0 is 17.635220542655293\n",
            "Validation loss in epoch 0 is 25.00630035667764\n",
            "Validation accuracy: 78.71379089355469, Train accuracy: 96.88119506835938\n",
            "File saved successfully\n",
            "\n",
            "Epoch [2/10], Step [100/584] Loss: 0.1152\n",
            "Epoch [2/10], Step [200/584] Loss: 0.0985\n",
            "Epoch [2/10], Step [300/584] Loss: 0.0030\n",
            "Epoch [2/10], Step [400/584] Loss: 0.0045\n",
            "Epoch [2/10], Step [500/584] Loss: 0.0012\n",
            "Training loss in epoch 1 is 1.8869128634309589\n",
            "Validation loss in epoch 1 is 13.255416145120059\n",
            "Validation accuracy: 88.54637145996094, Train accuracy: 99.24441528320312\n",
            "File saved successfully\n",
            "\n",
            "Epoch [3/10], Step [100/584] Loss: 0.0018\n",
            "Epoch [3/10], Step [200/584] Loss: 0.0148\n",
            "Epoch [3/10], Step [300/584] Loss: 0.0444\n",
            "Epoch [3/10], Step [400/584] Loss: 0.0094\n",
            "Epoch [3/10], Step [500/584] Loss: 0.1295\n",
            "Training loss in epoch 2 is 0.9595900180906841\n",
            "Validation loss in epoch 2 is 16.273506951956172\n",
            "Validation accuracy: 86.36726379394531, Train accuracy: 99.39445495605469\n",
            "\n",
            "\n",
            "Epoch [4/10], Step [100/584] Loss: 0.1249\n",
            "Epoch [4/10], Step [200/584] Loss: 0.0026\n",
            "Epoch [4/10], Step [300/584] Loss: 0.0021\n",
            "Epoch [4/10], Step [400/584] Loss: 0.0004\n",
            "Epoch [4/10], Step [500/584] Loss: 0.0002\n",
            "Training loss in epoch 3 is 0.5283644672835309\n",
            "Validation loss in epoch 3 is 24.90150856228861\n",
            "Validation accuracy: 85.43714904785156, Train accuracy: 99.29800415039062\n",
            "\n",
            "\n",
            "Epoch [5/10], Step [100/584] Loss: 0.0017\n",
            "Epoch [5/10], Step [200/584] Loss: 0.0014\n",
            "Epoch [5/10], Step [300/584] Loss: 0.0122\n",
            "Epoch [5/10], Step [400/584] Loss: 0.0015\n",
            "Epoch [5/10], Step [500/584] Loss: 0.0003\n",
            "Training loss in epoch 4 is 0.36578631093788705\n",
            "Validation loss in epoch 4 is 20.842081437755102\n",
            "Validation accuracy: 87.96173095703125, Train accuracy: 99.84459686279297\n",
            "\n",
            "\n",
            "Epoch [6/10], Step [100/584] Loss: 0.0006\n",
            "Epoch [6/10], Step [200/584] Loss: 0.0000\n",
            "Epoch [6/10], Step [300/584] Loss: 0.0020\n",
            "Epoch [6/10], Step [400/584] Loss: 0.0589\n",
            "Epoch [6/10], Step [500/584] Loss: 0.0476\n",
            "Training loss in epoch 5 is 0.1943065973557052\n",
            "Validation loss in epoch 5 is 22.61017422866717\n",
            "Validation accuracy: 88.33377838134766, Train accuracy: 99.89818572998047\n",
            "\n",
            "\n",
            "Epoch [7/10], Step [100/584] Loss: 0.0004\n",
            "Epoch [7/10], Step [200/584] Loss: 0.0000\n",
            "Epoch [7/10], Step [300/584] Loss: 0.0460\n",
            "Epoch [7/10], Step [400/584] Loss: 0.0365\n",
            "Epoch [7/10], Step [500/584] Loss: 0.0022\n",
            "Training loss in epoch 6 is 0.3388915065815865\n",
            "Validation loss in epoch 6 is 19.696056263666346\n",
            "Validation accuracy: 88.78553771972656, Train accuracy: 99.9035415649414\n",
            "File saved successfully\n",
            "\n",
            "Epoch [8/10], Step [100/584] Loss: 0.0002\n",
            "Epoch [8/10], Step [200/584] Loss: 0.0008\n",
            "Epoch [8/10], Step [300/584] Loss: 0.0005\n",
            "Epoch [8/10], Step [400/584] Loss: 0.0003\n",
            "Epoch [8/10], Step [500/584] Loss: 0.0000\n",
            "Training loss in epoch 7 is 0.2060323296996662\n",
            "Validation loss in epoch 7 is 17.889316781064377\n",
            "Validation accuracy: 88.75897216796875, Train accuracy: 99.97320556640625\n",
            "\n",
            "\n",
            "Epoch [9/10], Step [100/584] Loss: 0.0001\n",
            "Epoch [9/10], Step [200/584] Loss: 0.0036\n",
            "Epoch [9/10], Step [300/584] Loss: 0.0010\n",
            "Epoch [9/10], Step [400/584] Loss: 0.0000\n",
            "Epoch [9/10], Step [500/584] Loss: 0.0002\n",
            "Training loss in epoch 8 is 0.11588973374361357\n",
            "Validation loss in epoch 8 is 22.18989697697075\n",
            "Validation accuracy: 89.21073913574219, Train accuracy: 99.93569946289062\n",
            "File saved successfully\n",
            "\n",
            "Epoch [10/10], Step [100/584] Loss: 0.0011\n",
            "Epoch [10/10], Step [200/584] Loss: 0.0000\n",
            "Epoch [10/10], Step [300/584] Loss: 0.0000\n",
            "Epoch [10/10], Step [400/584] Loss: 0.0000\n",
            "Epoch [10/10], Step [500/584] Loss: 0.0000\n",
            "Training loss in epoch 9 is 0.06740180801527774\n",
            "Validation loss in epoch 9 is 25.572967967263505\n",
            "Validation accuracy: 88.06803131103516, Train accuracy: 99.81243896484375\n",
            "\n",
            "\n",
            "Best epoch:8, Best Validtion accuracy:89.21073913574219\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-5S6YCX1i_b",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGOmYeCv1o1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92ec1bc8-d732-4aae-e5ee-bdcf3f30946c"
      },
      "source": [
        "# selecting and loading weights of a trainied models\n",
        "\n",
        "def select_model(inp,CNNClassifier,extra_layer):\n",
        "  \n",
        "  if inp==1:\n",
        "    model=CNNClassifier()\n",
        "    name=\"CNN\"\n",
        "  elif inp==2:\n",
        "    model=nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=False), extra_layer)\n",
        "    name=\"VGG16\"\n",
        "  elif inp==3:\n",
        "    model = nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False), extra_layer)\n",
        "    name=\"RESNET18\"\n",
        "  elif inp==4:\n",
        "    model = nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', pretrained=False), extra_layer)\n",
        "    name=\"RESNET34\"\n",
        "  elif inp==5:\n",
        "    model=nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=False), extra_layer)\n",
        "    name=\"ALEXNET\"\n",
        "  elif inp==6:\n",
        "    model=nn.Sequential(torch.hub.load('pytorch/vision:v0.6.0', 'mobilenet_v2', pretrained=False), extra_layer)\n",
        "    name=\"MOBILENET_V2\"\n",
        "  return model, name \n",
        "\n",
        "print(\"-------------------Select Model----------------------\")\n",
        "print(\"Input '1' for CNNClassifier\")\n",
        "print(\"Input '2' for VGG16\")\n",
        "print(\"Input '3' for Resnet18\")\n",
        "print(\"Input '4' for Resnet34\")\n",
        "print(\"Input '5' for Alexnet\")\n",
        "print(\"Input '6' for Mobilenet_v2\")\n",
        "inp=int(input(\"Your input---> \"))\n",
        "print(\"-----------------------------------------------------------\")\n",
        "\n",
        "model, name=select_model(inp,CNNClassifier(),extra_layer())\n",
        "model.to(device)\n",
        "print(f\"selected model---> {name}\\n  {model}\")\n",
        "\n",
        "\n",
        "print(\"-------------------Load Weights---------------------\")\n",
        "\n",
        "loc=input(f\"enter the location of pkl file for model {name}\")\n",
        "model.load_state_dict(torch.load(loc))\n",
        "\n",
        "\n",
        "print(\"-----------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------Select Model----------------------\n",
            "Input '1' for CNNClassifier\n",
            "Input '2' for VGG16\n",
            "Input '3' for Resnet18\n",
            "Input '4' for Resnet34\n",
            "Input '5' for Alexnet\n",
            "Input '6' for Mobilenet_v2\n",
            "Your input---> 2\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "selected model---> VGG16\n",
            "  Sequential(\n",
            "  (0): VGG(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (6): ReLU(inplace=True)\n",
            "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): ReLU(inplace=True)\n",
            "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): ReLU(inplace=True)\n",
            "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (15): ReLU(inplace=True)\n",
            "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (18): ReLU(inplace=True)\n",
            "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (20): ReLU(inplace=True)\n",
            "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (22): ReLU(inplace=True)\n",
            "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (25): ReLU(inplace=True)\n",
            "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (27): ReLU(inplace=True)\n",
            "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (29): ReLU(inplace=True)\n",
            "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): Dropout(p=0.5, inplace=False)\n",
            "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (1): extra_layer(\n",
            "    (fc): Sequential(\n",
            "      (0): ReLU()\n",
            "      (1): Linear(in_features=1000, out_features=512, bias=True)\n",
            "      (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "-------------------Load Weights---------------------\n",
            "enter the location of pkl file for model VGG16/content/gdrive/My Drive/AML/Project/Coding/With weights/Weights/VGG16 epoch 1\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgz_fYSJCgE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=1e-2\n",
        "criterion=nn.CrossEntropyLoss(reduction='mean') # loss criterion \n",
        "optim=torch.optim.SGD(model.parameters(), lr=lr) # optimizer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ufwIXq-Rv9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a4a9c2f-1c1c-40e1-e25f-b65227b90aa8"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  val_acc,val_loss = get_model_acc_and_loss(True, model, criterion, val_loader)\n",
        "  print(f'Validation accuracy: {val_acc}, & Validation loss: {val_loss}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy: 88.57295227050781, & Validation loss: 47.23905520886183\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}